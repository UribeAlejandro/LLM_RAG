{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:41.989313Z",
     "start_time": "2023-12-07T20:10:41.942628Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from langchain.llms import GPT4All\n",
    "import pinecone\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from src.constants import EVAL_QUESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:41.989709Z",
     "start_time": "2023-12-07T20:10:41.945868Z"
    }
   },
   "id": "659d8a5803ae0671"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key = os.getenv(\"PINECONE_API_KEY\"),\n",
    "    environment = os.getenv(\"PINECONE_ENV\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:42.220132Z",
     "start_time": "2023-12-07T20:10:41.963717Z"
    }
   },
   "id": "bbca937d760c4ad4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "index = pinecone.Index(\"clementine-loka\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:42.225605Z",
     "start_time": "2023-12-07T20:10:42.222290Z"
    }
   },
   "id": "ef5c7d5148d00b5"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# llm = GPT4All(model=\"models/orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "llm = GPT4All(model=\"models/gpt4all-falcon-q4_0.gguf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:46.100563Z",
     "start_time": "2023-12-07T20:10:42.229369Z"
    }
   },
   "id": "ee69ec26687609ad"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def load_retriever(persist_directory):\n",
    "    embeddings = GPT4AllEmbeddings()\n",
    "    vectorstore = Pinecone.from_existing_index(\"clementine-loka\", embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:46.101603Z",
     "start_time": "2023-12-07T20:10:46.099868Z"
    }
   },
   "id": "dfb428c88a40ab71"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "embeddings = GPT4AllEmbeddings()\n",
    "vectorstore = Pinecone.from_existing_index(\"clementine-loka\", embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:46.645677Z",
     "start_time": "2023-12-07T20:10:46.102168Z"
    }
   },
   "id": "f6e3e7a577f7eb6f"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:46.649249Z",
     "start_time": "2023-12-07T20:10:46.645974Z"
    }
   },
   "id": "b8757498a7611bc9"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "'SageMaker geospatial capabilities roles\\n\\nAs a managed service, Amazon SageMaker geospatial capabilities perform operations on your behalf on the AWS hardware that is managed by SageMaker. It can perform only operations that the user permits.\\n\\nA user can grant these permissions with an IAM role (referred to as an execution role).\\n\\nTo create and use a locally available execution role, you can use the following procedures.\\n\\nCreate an execution role\\n\\nTo work with SageMaker geospatial capabilities you need to setup a user role and an execution role. A user role is an AWS identity with permission policies that determine what the user can and can not do within AWS. An execution role is an IAM role that grants the service permission to access your AWS resources. An execution role consists of permissions and trust policy. The trust policy specifies which principals have the permission to assume the role.'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"SageMaker Geospatial capabilities\")[0].page_content#metadata[\"source\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:47.420969Z",
     "start_time": "2023-12-07T20:10:46.648777Z"
    }
   },
   "id": "b1d2492a97f7234c"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "ANSWERS = [\n",
    "    \"Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models\",\n",
    "    \"All supported AWS regions except China (Beijing), Asia Pacific (Jakarta), Middle East (UAE), Asia Pacific (Hyderabad), Asia Pacific (Melbourne), AWS GovCloud (US-East), AWS GovCloud (US-West), Europe (Spain), China (Ningxia), Europe (Zurich) Region\",\n",
    "    \"Checks whether AWS Key Management Service (KMS) key is configured for an Amazon SageMaker endpoint configuration. The rule is NON_COMPLIANT if 'KmsKeyId' is not specified for the Amazon SageMaker endpoint configuration.\",\n",
    "    \"SageMaker geospatial capabilities rolesAs a managed service, Amazon SageMaker geospatial capabilities perform operations on your behalf on the AWS hardware that is managed by SageMaker. It can perform only operations that the user permits.\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:47.426065Z",
     "start_time": "2023-12-07T20:10:47.422127Z"
    }
   },
   "id": "267fbbec0365fb63"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "retriever=load_retriever(\"\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type='stuff',\n",
    "    verbose=True,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:10:48.006402Z",
     "start_time": "2023-12-07T20:10:47.426338Z"
    }
   },
   "id": "d4198b2d57af287"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "' The answer is \"sagemaker-notebook-instance-root-access-check\".'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are all AWS regions where SageMaker is available?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:11:06.938321Z",
     "start_time": "2023-12-07T20:10:48.011108Z"
    }
   },
   "id": "3347f8035f537a0e"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/07 17:11:12 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/cg/6bd366p92fbb82w4cdb91yd40000gn/T/tmpq6fh6yqv/model, flavor: langchain), fall back to return ['langchain==0.0.346']. Set logging level to DEBUG to see the full traceback.\n",
      "/Users/uribe/.local/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/uribe/.local/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "604550bff127408fa00049be5c292b03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: failed to mlock 46448640-byte buffer (after previously locking 3614998528 bytes): Resource temporarily unavailable\n",
      "/Users/uribe/miniconda3/lib/python3.11/site-packages/mlflow/data/digest_utils.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n",
      "/Users/uribe/miniconda3/lib/python3.11/site-packages/mlflow/models/evaluation/base.py:414: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(_hash_array_like_element_as_bytes)\n",
      "2023/12/07 17:11:24 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/12/07 17:11:24 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/07 17:12:29 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/07 17:12:30 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: toxicity\n",
      "2023/12/07 17:12:30 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: ari_grade_level\n",
      "2023/12/07 17:12:30 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: flesch_kincaid_grade_level\n",
      "2023/12/07 17:12:30 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: exact_match\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt4all-falcon\"\n",
    "persist_dir=\"database\"\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "\n",
    "with mlflow.start_run(run_name=\"log_model_\"+ model_name):\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    \n",
    "    logged_model = mlflow.langchain.log_model(\n",
    "        qa_chain, \n",
    "        artifact_path=\"model\",\n",
    "        loader_fn=load_retriever,\n",
    "        persist_dir=persist_dir,\n",
    "    )\n",
    "    \n",
    "    questions = pd.DataFrame({\"query\": EVAL_QUESTIONS,\n",
    "                              \"answer\": ANSWERS})\n",
    "    generated = mlflow.evaluate(\n",
    "        model=logged_model.model_uri,\n",
    "        # model_type=\"question-answering\",\n",
    "        data=questions,\n",
    "        targets=\"answer\",\n",
    "        extra_metrics=[\n",
    "            mlflow.metrics.toxicity(), \n",
    "            mlflow.metrics.latency(), \n",
    "            mlflow.metrics.ari_grade_level(), \n",
    "            mlflow.metrics.flesch_kincaid_grade_level(),\n",
    "            mlflow.metrics.exact_match(),\n",
    "        ],\n",
    "    )\n",
    "    mlflow.end_run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:12:33.981174Z",
     "start_time": "2023-12-07T20:11:06.944168Z"
    }
   },
   "id": "7556b0ac9cfbf350"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T20:12:33.983345Z",
     "start_time": "2023-12-07T20:12:33.979398Z"
    }
   },
   "id": "f5dd90e0b786395a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
